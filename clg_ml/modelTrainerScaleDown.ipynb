{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a9936ba-5255-41ef-b67a-433d65d9545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61042df8-12d7-462a-900a-a4fe7fcc3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "INITIAL_LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55ed7bd4-14b6-4a31-b3bc-4b11649c8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_dir = '/mnt/k/ml/clg_ml/domain_classification/train'\n",
    "val_dir = '/mnt/k/ml/clg_ml/domain_classification/val'\n",
    "test_dir = '/mnt/k/ml/clg_ml/domain_classification/test'\n",
    "MODEL_PATH = 'domain_classifier_best.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "626e2e06-0e07-4ce3-b2ea-53f881c447eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DATA PREPARATION ====================\n",
    "def create_data_generators():\n",
    "    \"\"\"Create data generators with augmentation for training\"\"\"\n",
    "    \n",
    "    # Training data generator with augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Validation and test data generator (only rescaling)\n",
    "    val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Create generators\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_generator = val_test_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    test_generator = val_test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator, test_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3de68301-1cf4-4219-8df6-ce12b82f7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MODEL ARCHITECTURE ====================\n",
    "def build_domain_classifier():\n",
    "    \"\"\"Build DenseNet121 based domain classifier\"\"\"\n",
    "    \n",
    "    # Load pre-trained DenseNet121\n",
    "    base_model = DenseNet121(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add custom head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(2, activation='softmax', name='domain_output')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4bac594-76b7-428f-ac61-0c4b40020f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TRAINING STRATEGY ====================\n",
    "def get_callbacks():\n",
    "    \"\"\"Define training callbacks\"\"\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        MODEL_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return [checkpoint, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c25cc306-5ea6-40da-b3cc-feac14551f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_domain_classifier():\n",
    "    \"\"\"Train the domain classifier with two-phase approach\"\"\"\n",
    "    \n",
    "    # Create data generators\n",
    "    train_gen, val_gen, test_gen = create_data_generators()\n",
    "    \n",
    "    # Build model\n",
    "    model, base_model = build_domain_classifier()\n",
    "    \n",
    "    # Phase 1: Train only the head\n",
    "    print(\"Phase 1: Training classifier head...\")\n",
    "    model.compile(\n",
    "        optimizer=Adam(INITIAL_LR),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history_head = model.fit(\n",
    "        train_gen,\n",
    "        epochs=10,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=get_callbacks(),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Phase 2: Fine-tune deeper layers\n",
    "    print(\"Phase 2: Fine-tuning deeper layers...\")\n",
    "    \n",
    "    # Unfreeze last 50 layers of base model\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-50]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(INITIAL_LR/10),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history_full = model.fit(\n",
    "        train_gen,\n",
    "        epochs=EPOCHS,\n",
    "        initial_epoch=history_head.epoch[-1] + 1,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=get_callbacks(),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, train_gen, val_gen, test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa7044b2-2cab-4829-8f34-d60f9ed9e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== EVALUATION ====================\n",
    "def evaluate_model(model, test_generator):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    # Get true labels and predictions\n",
    "    test_generator.reset()\n",
    "    y_true = test_generator.classes\n",
    "    y_pred_probs = model.predict(test_generator)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    # Classification report\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nCONFUSION MATRIX:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Overall accuracy\n",
    "    test_accuracy = np.sum(y_pred == y_true) / len(y_true)\n",
    "    print(f\"\\nOverall Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_accuracy, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3d4da80-e02d-49bb-a9c7-85816a3d80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION ====================\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6dc365a0-d581-4284-997d-6cdb6cfad6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_google_images(model_path, google_images_dir):\n",
    "    \"\"\"\n",
    "    Classify images from Google images directory and show probabilities\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Class names\n",
    "    class_names = ['oral_disorder', 'skin_diseases']\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(google_images_dir) \n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp'))]\n",
    "    \n",
    "    print(f\"Classifying {len(image_files)} images from Google directory...\")\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(google_images_dir, img_file)\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = image.load_img(img_path, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = model.predict(img_array, verbose=0)\n",
    "        predicted_class = class_names[np.argmax(predictions[0])]\n",
    "        confidence = np.max(predictions[0])\n",
    "        \n",
    "        print(f\"{img_file}: Predicted → {predicted_class} ({confidence*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f890745-7ddb-4934-acfe-d2eb6e0fd784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Domain Classifier Training...\n",
      "Training samples: 4361 skin + 4789 oral\n",
      "Validation samples: 531 skin + 282 oral\n",
      "Found 9148 images belonging to 2 classes.\n",
      "Found 813 images belonging to 2 classes.\n",
      "Found 826 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759945931.275142     804 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3618 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Training classifier head...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/k/ml/clg_ml/venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 23:22:24.015468: I external/local_xla/xla/service/service.cc:163] XLA service 0x7ff8d0002eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-08 23:22:24.015598: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 6GB Laptop GPU, Compute Capability 8.6\n",
      "2025-10-08 23:22:24.564034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-08 23:22:27.225221: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91300\n",
      "2025-10-08 23:22:27.812677: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-08 23:22:27.812740: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-08 23:22:27.263928: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13442', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-10-08 23:22:28.044575: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13459', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "I0000 00:00:1759945970.790695    1726 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563ms/step - accuracy: 0.8321 - loss: 0.3754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 23:25:35.236835: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3392', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.97294, saving model to domain_classifier_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 664ms/step - accuracy: 0.9180 - loss: 0.1992 - val_accuracy: 0.9729 - val_loss: 0.0632 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.9742 - loss: 0.0688\n",
      "Epoch 2: val_accuracy improved from 0.97294 to 0.98155, saving model to domain_classifier_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 324ms/step - accuracy: 0.9779 - loss: 0.0616 - val_accuracy: 0.9815 - val_loss: 0.0370 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - accuracy: 0.9843 - loss: 0.0422\n",
      "Epoch 3: val_accuracy improved from 0.98155 to 0.99016, saving model to domain_classifier_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 313ms/step - accuracy: 0.9846 - loss: 0.0434 - val_accuracy: 0.9902 - val_loss: 0.0220 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.9814 - loss: 0.0458\n",
      "Epoch 4: val_accuracy did not improve from 0.99016\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 325ms/step - accuracy: 0.9843 - loss: 0.0411 - val_accuracy: 0.9877 - val_loss: 0.0277 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - accuracy: 0.9872 - loss: 0.0347\n",
      "Epoch 5: val_accuracy did not improve from 0.99016\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 343ms/step - accuracy: 0.9883 - loss: 0.0332 - val_accuracy: 0.9902 - val_loss: 0.0207 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - accuracy: 0.9899 - loss: 0.0280\n",
      "Epoch 6: val_accuracy improved from 0.99016 to 0.99262, saving model to domain_classifier_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 342ms/step - accuracy: 0.9896 - loss: 0.0284 - val_accuracy: 0.9926 - val_loss: 0.0162 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.9937 - loss: 0.0209\n",
      "Epoch 7: val_accuracy improved from 0.99262 to 0.99508, saving model to domain_classifier_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 311ms/step - accuracy: 0.9917 - loss: 0.0247 - val_accuracy: 0.9951 - val_loss: 0.0103 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.9910 - loss: 0.0294\n",
      "Epoch 8: val_accuracy improved from 0.99508 to 0.99754, saving model to domain_classifier_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 328ms/step - accuracy: 0.9919 - loss: 0.0239 - val_accuracy: 0.9975 - val_loss: 0.0118 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.9921 - loss: 0.0216\n",
      "Epoch 9: val_accuracy did not improve from 0.99754\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 332ms/step - accuracy: 0.9927 - loss: 0.0213 - val_accuracy: 0.9963 - val_loss: 0.0092 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - accuracy: 0.9928 - loss: 0.0225\n",
      "Epoch 10: val_accuracy did not improve from 0.99754\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 337ms/step - accuracy: 0.9925 - loss: 0.0238 - val_accuracy: 0.9975 - val_loss: 0.0074 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Phase 2: Fine-tuning deeper layers...\n",
      "Epoch 11/50\n",
      "\u001b[1m283/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 366ms/step - accuracy: 0.9931 - loss: 0.0209\n",
      "Epoch 11: val_accuracy improved from None to 0.99262, saving model to domain_classifier_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 427ms/step - accuracy: 0.9943 - loss: 0.0183 - val_accuracy: 0.9926 - val_loss: 0.0154 - learning_rate: 1.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.9938 - loss: 0.0179\n",
      "Epoch 12: val_accuracy did not improve from 0.99262\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 324ms/step - accuracy: 0.9934 - loss: 0.0195 - val_accuracy: 0.9926 - val_loss: 0.0136 - learning_rate: 1.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.9938 - loss: 0.0171\n",
      "Epoch 13: val_accuracy improved from 0.99262 to 0.99385, saving model to domain_classifier_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 354ms/step - accuracy: 0.9938 - loss: 0.0173 - val_accuracy: 0.9938 - val_loss: 0.0103 - learning_rate: 1.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m280/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 317ms/step - accuracy: 0.9908 - loss: 0.0229\n",
      "Epoch 14: val_accuracy did not improve from 0.99385\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 325ms/step - accuracy: 0.9929 - loss: 0.0207 - val_accuracy: 0.9938 - val_loss: 0.0110 - learning_rate: 1.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - accuracy: 0.9942 - loss: 0.0144\n",
      "Epoch 15: val_accuracy did not improve from 0.99385\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 354ms/step - accuracy: 0.9951 - loss: 0.0150 - val_accuracy: 0.9926 - val_loss: 0.0150 - learning_rate: 1.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.9974 - loss: 0.0104\n",
      "Epoch 16: val_accuracy did not improve from 0.99385\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 317ms/step - accuracy: 0.9958 - loss: 0.0124 - val_accuracy: 0.9926 - val_loss: 0.0128 - learning_rate: 1.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.9942 - loss: 0.0186\n",
      "Epoch 17: val_accuracy did not improve from 0.99385\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 319ms/step - accuracy: 0.9946 - loss: 0.0175 - val_accuracy: 0.9926 - val_loss: 0.0140 - learning_rate: 1.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 0.9950 - loss: 0.0134\n",
      "Epoch 18: val_accuracy did not improve from 0.99385\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 316ms/step - accuracy: 0.9955 - loss: 0.0150 - val_accuracy: 0.9938 - val_loss: 0.0124 - learning_rate: 1.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.9942 - loss: 0.0142\n",
      "Epoch 19: val_accuracy did not improve from 0.99385\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 349ms/step - accuracy: 0.9954 - loss: 0.0136 - val_accuracy: 0.9938 - val_loss: 0.0117 - learning_rate: 2.0000e-06\n",
      "Epoch 20/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.9959 - loss: 0.0111\n",
      "Epoch 20: val_accuracy did not improve from 0.99385\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 328ms/step - accuracy: 0.9954 - loss: 0.0149 - val_accuracy: 0.9938 - val_loss: 0.0117 - learning_rate: 2.0000e-06\n",
      "Epoch 21/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.9951 - loss: 0.0188\n",
      "Epoch 21: val_accuracy did not improve from 0.99385\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 329ms/step - accuracy: 0.9946 - loss: 0.0155 - val_accuracy: 0.9938 - val_loss: 0.0130 - learning_rate: 2.0000e-06\n",
      "Epoch 22/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 0.9943 - loss: 0.0182\n",
      "Epoch 22: val_accuracy did not improve from 0.99385\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 340ms/step - accuracy: 0.9943 - loss: 0.0152 - val_accuracy: 0.9938 - val_loss: 0.0111 - learning_rate: 2.0000e-06\n",
      "Epoch 23/50\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.9961 - loss: 0.0110\n",
      "Epoch 23: val_accuracy did not improve from 0.99385\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 3.999999989900971e-07.\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 345ms/step - accuracy: 0.9961 - loss: 0.0123 - val_accuracy: 0.9938 - val_loss: 0.0120 - learning_rate: 2.0000e-06\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Evaluating on test set...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step\n",
      "\n",
      "==================================================\n",
      "CLASSIFICATION REPORT\n",
      "==================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "oral_disorder       0.99      1.00      0.99       282\n",
      "skin_diseases       1.00      0.99      1.00       544\n",
      "\n",
      "     accuracy                           1.00       826\n",
      "    macro avg       0.99      1.00      0.99       826\n",
      " weighted avg       1.00      1.00      1.00       826\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[282   0]\n",
      " [  4 540]]\n",
      "\n",
      "Overall Test Accuracy: 0.9952\n",
      "\n",
      "Loading best model for final evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 449ms/step\n",
      "\n",
      "==================================================\n",
      "CLASSIFICATION REPORT\n",
      "==================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "oral_disorder       0.99      1.00      0.99       282\n",
      "skin_diseases       1.00      0.99      1.00       544\n",
      "\n",
      "     accuracy                           1.00       826\n",
      "    macro avg       0.99      1.00      0.99       826\n",
      " weighted avg       1.00      1.00      1.00       826\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[282   0]\n",
      " [  4 540]]\n",
      "\n",
      "Overall Test Accuracy: 0.9952\n",
      "\n",
      "🎯 Final Domain Classifier Performance: 0.9952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying 8 images from Google directory...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m final_accuracy, _, _ \u001b[38;5;241m=\u001b[39m evaluate_model(best_model, test_gen)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎯 Final Domain Classifier Performance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mclassify_google_images\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdomain_classifier_best.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/k/ml/clg_ml/imgs_from_google/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 21\u001b[0m, in \u001b[0;36mclassify_google_images\u001b[0;34m(model_path, google_images_dir)\u001b[0m\n\u001b[1;32m     18\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(google_images_dir, img_file)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load and preprocess image\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241m.\u001b[39mload_img(img_path, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n\u001b[1;32m     22\u001b[0m img_array \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n\u001b[1;32m     23\u001b[0m img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "# ==================== MAIN EXECUTION ====================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Domain Classifier Training...\")\n",
    "    print(f\"Training samples: {len(os.listdir(os.path.join(train_dir, 'skin_diseases')))} skin + {len(os.listdir(os.path.join(train_dir, 'oral_disorder')))} oral\")\n",
    "    print(f\"Validation samples: {len(os.listdir(os.path.join(val_dir, 'skin_diseases')))} skin + {len(os.listdir(os.path.join(val_dir, 'oral_disorder')))} oral\")\n",
    "    \n",
    "    # Train the model\n",
    "    model, train_gen, val_gen, test_gen = train_domain_classifier()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, y_true, y_pred = evaluate_model(model, test_gen)\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    print(\"\\nLoading best model for final evaluation...\")\n",
    "    best_model = tf.keras.models.load_model(MODEL_PATH)\n",
    "    final_accuracy, _, _ = evaluate_model(best_model, test_gen)\n",
    "    \n",
    "    print(f\"\\n🎯 Final Domain Classifier Performance: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85437bb4-83f4-4da6-a3a4-79050e52d137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying 12 images from Google directory...\n",
      "be_ke_le.jpeg: Predicted → skin_diseases (100.00%)\n",
      "be_ke_le2.jpg: Predicted → skin_diseases (100.00%)\n",
      "ecz.webp: Predicted → skin_diseases (98.70%)\n",
      "ecz2.jpg: Predicted → skin_diseases (99.31%)\n",
      "hypo.jpeg: Predicted → oral_disorder (100.00%)\n",
      "hypo2.jpg: Predicted → oral_disorder (100.00%)\n",
      "hypo3.jpg: Predicted → oral_disorder (99.90%)\n",
      "mo_ul.jpeg: Predicted → oral_disorder (100.00%)\n",
      "mo_ul2.jpg: Predicted → oral_disorder (99.99%)\n",
      "mo_ul3.jpg: Predicted → oral_disorder (99.97%)\n",
      "pso.webp: Predicted → skin_diseases (99.99%)\n",
      "pso2.jpg: Predicted → skin_diseases (100.00%)\n"
     ]
    }
   ],
   "source": [
    "classify_google_images('domain_classifier_best.h5', '/mnt/k/ml/clg_ml/imgs_from_google/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df28d9-357a-469e-86d8-6671d4ab61d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f93dd-c97e-46e0-8305-cebe678fceea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04af732-a5e8-4c12-a7b8-bf7ba4e253a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0ea5a-4213-4f58-aa75-754417b709e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3e28a-c4de-49c9-93c6-8f1cf16b3d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4746a-b84b-456f-a906-2d48d47e39be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a50ff-efe3-4755-83b1-aa33c4964668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad911542-e5aa-48dc-a892-3cc441a16ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec5170-24de-4768-8eda-d5c4dbb83701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c2a5e-c9e9-4332-80ce-dcc9e50edba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc2d4a-9b4d-4c4f-89cc-f6a60aaa1229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3bf525-7a76-4499-9982-3d52a3083644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3031d91-10a6-4367-a047-7e4a8cd8d3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ffda0-cfd7-42dd-bad1-8af4c86d7ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b5dd8-59b9-46c9-b8c9-65dc71e24bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a05987c-4263-411b-a832-c68fd9ebb8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b680291-07a6-4dd5-a176-f145b5cfb296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ad382-9201-4eea-a777-8892838302e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794e83c-bd8b-4775-ba52-4c903e997dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680df89-c69a-404f-8e25-8624fd0f4d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a50baa-7fa7-4058-9ed9-6236fe82ba15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb6e95-d6db-43ba-8180-8cc3c78cc35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c78f5-7fbe-4190-836e-a5fe0a5c4228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0581edb-a212-4b30-bdae-53243b88dfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc48cd3-9b2c-40e0-91b2-7c762c161f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d279c-d1c2-48c0-aad4-4e8bdc4a4073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416b488-c389-4de7-a50b-0567529f8bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
